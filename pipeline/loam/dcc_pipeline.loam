import qc_params._
import params._
import binaries._
import cloud_helpers._
import scripts._
import store_helpers._
import loamstream.model.Store
import loamstream.conf.DataConfig
import scala.io.Source
import loamstream.googlecloud.HailSupport._

val config = loadDataConfig(dataConfigFile)

for (params <- config.getObjList("project")) {
  allQcSteps(params)
}

def allQcSteps(params: DataConfig): Unit = {
  val kgPurcellVcfName = params.getStr("kgPurcellVcfName")
  val kgPurcellVcfDir = params.getStr("kgPurcellVcfDir")
  val kgSampleName = params.getStr("kgSampleName")
  val kgSampleDir = params.getStr("kgSampleDir")
  val kgSampleId = params.getStr("kgSampleId")
  val kgSamplePop = params.getStr("kgSamplePop")
  val kgSampleGroup = params.getStr("kgSampleGroup")
  val kgVcfBaseWild = params.getStr("kgVcfBaseWild")
  val regionsExcludeName = params.getStr("regionsExcludeName")
  val regionsExcludeDir = params.getStr("regionsExcludeDir")

  // Project Settings
  val id = params.getStr("id")
  val vcf = store[VCF].at(params.getStr("vcf")).asInput
  val phenoName = params.getStr("phenoName")
  val phenoDir = params.getStr("phenoDir")
  val phenoId = params.getStr("phenoId")
  val phenoSrSex = params.getStr("phenoSrSex")
  val phenoSrRace = params.getStr("phenoSrRace")
  val phenoStatus = params.getStr("phenoStatus")

  // TODO make these values work with dynamic execution
  val nChr = endChr - startChr + 1

  // Alignment Step
  val harmRefVcfName = s"${id}.harm.ref.vcf.gz"
  val harmRefVcfTbiName = s"${id}.harm.ref.vcf.gz.tbi"

  // Filter Step
  val harmRefFiltPrefixName = s"${id}.harm.ref.filt"
  val harmRefFiltPrunedPrefixName = s"${harmRefFiltPrefixName}.pruned"

  // Kinship Step
  val kinPrefixName = s"${id}.kinship"

  // Ancestry PCA Step
  val harmRef1kgPrefixName = s"${id}.harm.ref.1kg"

  // Ancestry Cluster Step
  val ancestryInferredName = s"${id}.ancestry.inferred.tsv"

  // Sample QC Stats Calculation Step
  val sampleqcStatsName = s"${id}.sampleqc.stats.tsv"

  // Sample QC Individual Stats Clustering Step
  val sampleqcStatsAdjOutliersTableName = s"${id}.sampleqc.outliers.tsv"

  // Filter Clean Step
  val cleanPrefixName = s"${id}.clean"

  object Local {
    // Inputs
    val kgPurcellVcf = store[VCF].at(path(kgPurcellVcfDir) / kgPurcellVcfName).asInput
    val kgSample = store[TXT].at(path(kgSampleDir) / kgSampleName).asInput
    val regionsExclude = store[TXT].at(path(regionsExcludeDir) / regionsExcludeName).asInput
    val pheno = store[TXT].at(path(phenoDir) / phenoName).asInput

    // Alignment Step
    val rawChrs: Array[Seq[Store[TXT]]] = Array.ofDim(nChr)
    val harmChrsPrefix: Array[Path] = Array.ofDim(nChr)
    val harmChrs: Array[Seq[Store[TXT]]] = Array.ofDim(nChr)
    val harmPrefix = localOutDir / s"${id}.harm"
    val harm = bedBimFam(harmPrefix)
    val harmRefPrefix = localOutDir / s"${id}.harm.ref"
    val harmRefVcf = store[VCF].at(localOutDir / harmRefVcfName)
    val harmRefVcfTbi = store[VCF].at(localOutDir / harmRefVcfTbiName)
    val kgVcfChrs: Array[Store[VCF]] = Array.ofDim(nChr)
    val harmVarIdUpdates: Array[Store[TXT]] = Array.ofDim(nChr)
    val harmVarSnpLogs: Array[Store[TXT]] = Array.ofDim(nChr)
    val harmMergeLines: Array[String] = Array.ofDim(nChr)
    val harmMergeList = store[TXT].at(localOutDir / s"${id}.harm.merge.txt")
    val harmForceA2 = store[TXT].at(localOutDir / s"${id}.harm.force_a2.txt")

    // Load Step

    // Filter Step
    val harmRefFiltPrefix = localOutDir / harmRefFiltPrefixName
    val harmRefFilt = bedBimFam(localOutDir / harmRefFiltPrefixName)
    val harmRefFiltPrunedVds = store[VCF].at(localOutDir / s"${harmRefFiltPrunedPrefixName}.vds")
    val harmRefFiltPrunedPrefix = localOutDir / harmRefFiltPrunedPrefixName
    val harmRefFiltPruned = bedBimFam(harmRefFiltPrunedPrefix)

    // Kinship Step
    val kinPrefix = localOutDir / kinPrefixName
    val kinLog = store[TXT].at(kinPrefix + ".log")
    val kinTmpDat = store[TXT].at(kinPrefix + "TMP.dat")
    val kinTmpPed = store[TXT].at(kinPrefix + "TMP.ped")
    val kinKin = store[TXT].at(kinPrefix + ".kin")
    val kinKin0 = store[TXT].at(kinPrefix + ".kin0")
    val kinKin0Related = store[TXT].at(kinPrefix + ".kin0.related")
    val kinFamsizes = store[TXT].at(kinPrefix + ".famsizes.tsv")

    // Ancestry PCA Step
    val ancestryPcaPrefix = localOutDir / s"${id}.ancestry.pca"
    val harmRef1kgPrefix = localOutDir / harmRef1kgPrefixName
    val harmRef1kg = bedBimFam(harmRef1kgPrefix)
    val harmRef1kgGds = store[TXT].at(s"$harmRef1kgPrefix.gds")
    val ancestryPcaLog = store[TXT].at(s"$ancestryPcaPrefix.log")
    val ancestryPcaScores = store[TXT].at(s"$ancestryPcaPrefix.scores.tsv")
    val ancestryPcaScoresPlots = store[TXT].at(s"$ancestryPcaPrefix.scores.plots.pdf")

    // Ancestry Cluster Step
    val ancestryClusterPrefix = localOutDir / s"${id}.ancestry.cluster"
    val ancestryClusterLog = store[TXT].at(ancestryClusterPrefix + ".log")
    val ancestryClusterFet = store[TXT].at(ancestryClusterPrefix + ".fet.1")
    val ancestryClusterClu = store[TXT].at(ancestryClusterPrefix + ".clu.1")
    val ancestryClusterKlg = store[TXT].at(ancestryClusterPrefix + ".klg.1")
    val ancestryClusterPlots = store[TXT].at(ancestryClusterPrefix + ".plots.pdf")
    val ancestryClusterPlotsCenters = store[TXT].at(ancestryClusterPrefix + ".plots.centers.pdf")
    val ancestryClusterPlotsNo1kg = store[TXT].at(ancestryClusterPrefix + ".plots.no_1kg.pdf")
    val ancestryClusterXtabs = store[TXT].at(ancestryClusterPrefix + ".xtabs")
    val ancestryClusterGroups = store[TXT].at(ancestryClusterPrefix + ".groups.tsv")
    val ancestryInferred = store[TXT].at(localOutDir / ancestryInferredName)

    // PCA Step
    val harmRefFiltPrunedPcaGds = store[TXT].at(s"$harmRefFiltPrunedPrefix.pca.gds")
    val ancestryOutliers = store[TXT].at(localOutDir / s"${id}.ancestry.outliers")
    val pcaLog = store[TXT].at(localOutDir / s"${id}.pca.log")
    val pcaScores = store[TXT].at(localOutDir / s"${id}.pca.scores.tsv")
    val pcaLoadings = store[TXT].at(localOutDir / s"${id}.pca.loadings.tsv")

    // Sample QC Stats Calculation Step
    val sampleqcStats = store[TXT].at(localOutDir / sampleqcStatsName)
    val sampleqcStatsAdj = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.tsv")
    val sampleqcStatsAdjCorrPlots = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.corr.pdf")
    val sampleqcStatsAdjPcaLoadings = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.pca.loadings.tsv")
    val sampleqcStatsAdjPcaScoresPlots = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.pca.plots.pdf")
    val sampleqcStatsAdjPcaScores = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.pca.scores.tsv")

    // Sample QC PCA Clustering Step
    val sampleqcStatsAdjClusterKlustakwikStores = KlustakwikStores(localOutDir / s"${id}.sampleqc.stats.adj.cluster")
    val sampleqcStatsAdjClusterOutliers = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.cluster.outliers")
    val sampleqcStatsAdjClusterPlots = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.cluster.plots.pdf")
    val sampleqcStatsAdjClusterXtabs = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.cluster.xtabs")
    val sampleqcStatsAdjClusterStripchart = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.cluster.stripchart.pdf")

    // Sample QC Individual Stats Clustering Step
    val sampleqcStatsAdjPrefix = localOutDir / s"${id}.sampleqc.stats.adj"
    val sampleqcStatsAdjIndWildcard = (localOutDir / s"${id}.sampleqc.stats.adj.[[STAR]].clu.1").toString.replace("[[STAR]]", "*")
    val sampleqcStatsAdjIndBoxplots = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.ind.boxplots.pdf")
    val sampleqcStatsAdjIndDiscreteness = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.ind.discreteness")
    val sampleqcStatsAdjOutliersTable = store[TXT].at(localOutDir / sampleqcStatsAdjOutliersTableName)
    val sampleqcStatsAdjStripchart = store[TXT].at(localOutDir / s"${id}.sampleqc.stats.adj.stripchart.pdf")

    // Filter Clean Step
    val cleanPrefix = localOutDir / cleanPrefixName
    val clean = bedBimFam(cleanPrefix)
    val cleanPcaGds = store[TXT].at(cleanPrefix + ".pca.gds")
    val cleanPcaScores = store[TXT].at(cleanPrefix + ".pca.scores.tsv")
    val cleanPcaScoresRdata = store[TXT].at(cleanPrefix + ".pca.scores.RData")
    val cleanPcaLog = store[TXT].at(cleanPrefix + ".pca.log")
    val cleanRelateGds = store[TXT].at(cleanPrefix + ".relate.gds")
    val cleanRelateScores = store[TXT].at(cleanPrefix + ".relate.scores.tsv")
    val cleanRelateLog = store[TXT].at(cleanPrefix + ".relate.log")
  }

  object Google {
    // Alignment Step
    val harmRefVcf = store[VCF].at(googleOutDir / harmRefVcfName)
    val harmRefVcfTbi = store[VCF].at(googleOutDir / harmRefVcfTbiName)

    // Load Step
    val harmRefVds = store[VCF].at(googleOutDir / s"${id}.harm.ref.vds")
    val regionsExclude = store[TXT].at(googleOutDir / regionsExcludeName)

    // Filter Step
    val harmRefFiltPrefix = googleOutDir / harmRefFiltPrefixName
    val harmRefFilt = bedBimFam(harmRefFiltPrefix)
    val harmRefFiltVariantQc = store[TXT].at(googleOutDir / s"${harmRefFiltPrefixName}.variantqc.tsv")
    val harmRefFiltVariantsPrunedIn = store[TXT].at(googleOutDir / s"${harmRefFiltPrunedPrefixName}.in")
    val harmRefFiltVds = store[VCF].at(googleOutDir / s"${harmRefFiltPrefixName}.vds")
    val harmRefFiltPrunedVds = store[VCF].at(googleOutDir / s"${harmRefFiltPrunedPrefixName}.vds")
    val harmRefFiltPrunedPrefix = googleOutDir / harmRefFiltPrunedPrefixName
    val harmRefFiltPruned = bedBimFam(harmRefFiltPrunedPrefix)

    // Kinship Step
    val kinPrefix = googleOutDir / kinPrefixName
    val kinKin0Related = store[TXT].at(kinPrefix + ".kin0.related")
    val kinFamsizes = store[TXT].at(kinPrefix + ".famsizes.tsv")

    // Ancestry PCA Step
    val kgPurcellVcf = store[VCF].at(googleOutDir / kgPurcellVcfName)
    val kgSample = store[TXT].at(googleOutDir / kgSampleName)
    val harmRef1kgPrefix = googleOutDir / harmRef1kgPrefixName
    val harmRef1kg = bedBimFam(harmRef1kgPrefix)

    // Sample QC Stats Calculation Step
    val sampleqcSexcheck = store[TXT].at(googleOutDir / s"${id}.sampleqc.sexcheck.tsv")
    val sampleqcSexcheckProblems = store[TXT].at(googleOutDir / s"${id}.sampleqc.sexcheck.problems.tsv")

    // Ancestry Cluster Step
    val ancestryInferred = store[TXT].at(googleOutDir / ancestryInferredName)

    // Sample QC Stats Calculation Step
    val sampleqcStats = store[TXT].at(googleOutDir / sampleqcStatsName)
    val pheno = store[TXT].at(googleOutDir / phenoName)

    // Sample QC Individual Stats Clustering Step
    val sampleqcStatsAdjOutliersTable = store[TXT].at(googleOutDir / sampleqcStatsAdjOutliersTableName)

    // Compile Sample Exclusions Step
    val finalSampleExclusions = store[TXT].at(googleOutDir / s"${id}.final.sample.exclusions")

    // Filter Clean Step
    val cleanPrefix = googleOutDir / s"${id}.clean"
    val clean = bedBimFam(cleanPrefix)
  }

  /**
   * Alignment Step
   *  Description: Align data strand to 1KG reference. Also, update reference allele and variant ID to match 1KG
   *  Requires: Plink1.9 and, at least, Genotype Harmonizer v1.4.18
   *  Input: $vcf, $kgVcfBaseWild (VCF files, all chromosomes)
   *  Output Needed: ${id}.chr${CHROMOSOME}.bed/bim/fam, ${id}.chr${CHROMOSOME}.harm.bed/bim/fam/log(/nosex?/hh?), merge.txt, force_a2.txt,
   *     ${id}.harm.sample, ${id}.chr${CHROMOSOME}.harm_idUpdates.txt, ${id}.chr${CHROMOSOME}.harm_snpLog.log
   *  Notes:
   *     Could also add --variants and --mafAlign as pipeline options, but for now these are static
   *     Ideally, this will be run in parallel by chromosome number
   */
  
  uger {
    for (i <- startChr to endChr) {
      val j = i - startChr
      val rawChrsName = localOutDir / s"${id}.chr$i"
      Local.rawChrs(j) = bedBimFam(rawChrsName)
      Local.harmChrsPrefix(j) = localOutDir / s"${id}.chr$i.harm"
      Local.harmChrs(j) = bedBimFam(Local.harmChrsPrefix(j))
      Local.kgVcfChrs(j) = store[VCF].at(kgVcfBaseWild.replace("[CHROMOSOME]", s"$i").replace("23.phase3_shapeit2_mvncall_integrated_v5a","X.phase3_shapeit2_mvncall_integrated_v1b") + ".vcf.gz").asInput
      Local.harmVarIdUpdates(j) = store[TXT].at(localOutDir / s"${id}.chr$i.harm_idUpdates.txt")
      Local.harmVarSnpLogs(j) = store[TXT].at(localOutDir / s"${id}.chr$i.harm_snpLog.log")
  
      cmd"""$binPlink --vcf $vcf --chr $i --keep-allele-order --make-bed --output-chr MT --out $rawChrsName"""
      .in(vcf)
      .out(Local.rawChrs(j))
  
      cmd"""$binGenotypeHarmonizer
        --input $rawChrsName
        --inputType PLINK_BED
        --output ${Local.harmChrsPrefix(j)}
        --outputType PLINK_BED
        --ref ${Local.kgVcfChrs(j)}
        --refType VCF
        --keep
        --update-id
        --variants 1000
        --mafAlign 0.1
        --update-id
        --update-reference-allele
        --debug"""
        .in(Local.rawChrs(j) :+ Local.kgVcfChrs(j))
        .out(Local.harmChrs(j) :+ Local.harmVarIdUpdates(j) :+ Local.harmVarSnpLogs(j))
  
      Local.harmMergeLines(j) = s"${Local.harmChrsPrefix(j) + s".$bed"} ${Local.harmChrsPrefix(j) + s".$bim"} ${Local.harmChrsPrefix(j) + s".$fam"}"
    }
  
    val harmMergeLinesConcat: String = Local.harmMergeLines.drop(1).mkString("\n") // Exclude first chrom
  
    cmd"""echo "$harmMergeLinesConcat" > ${Local.harmMergeList}""".out(Local.harmMergeList)
  
    cmd"""$binPlink --bfile ${Local.harmChrsPrefix(0)} --merge-list ${Local.harmMergeList} --make-bed --keep-allele-order --out ${Local.harmPrefix}"""
    .in(Local.harmChrs.flatten :+ Local.harmMergeList)
    .out(Local.harm)
  
    cmd"""awk '{print $$2,$$5}' ${Local.harmPrefix}.bim > ${Local.harmForceA2}"""
    .in(Local.harm)
    .out(Local.harmForceA2)
  
    cmd"""$binPlink --bfile ${Local.harmPrefix} --recode vcf-iid bgz --real-ref-alleles --a2-allele ${Local.harmForceA2} --out ${Local.harmRefPrefix}"""
    .in(Local.harm :+ Local.harmForceA2)
    .out(Local.harmRefVcf)
  
    cmd"""$binTabix -f -p vcf ${Local.harmRefVcf}"""
    .in(Local.harmRefVcf)
    .out(Local.harmRefVcfTbi)
  }
  
  /**
   * Load Step
   *  Description: Generate the Hail VDS from VCF file and a sample file containing population and sex information
   *  Requires: Hail, Java (version under which Hail was compiled)
   *  Input: $harmRefVcf
   *  Output Needed: ${id}.harm.ref.vds/, ${id}.harm.ref.vds.log
   *  Notes:
   *   Monomorphic variants are automatically removed during import into Hail
   */
  
  local {
    googleCopy(Local.harmRefVcf, Google.harmRefVcf)
    googleCopy(Local.harmRefVcfTbi, Google.harmRefVcfTbi)
  }

  google {
    hail"""$pyHailLoad
      --vcf-in $id ${Google.harmRefVcf}
	  --vds-out ${Google.harmRefVds}"""
	  .in(Google.harmRefVcf, Google.harmRefVcfTbi)
	  .out(Google.harmRefVds)
  }
  
  /**
   * Filter Step
   *  Description: Generate filtered and filtered/pruned filesets for QC
   *  Requires: Hail, Plink, Java (version under which Hail was compiled)
   *  Input: $harmRefVds, $regionsExclude
   *  Output: ${id}.filter.log, ${id}.variantqc.tsv, ${id}.filt.vds, ${id}.filt.pruned.vds, ${id}.filt.bed/bim/fam, ${id}.filt.prune.in, ${id}.filt.prune.out
   *  Notes:
   */

  local {
    googleCopy(Local.regionsExclude, Google.regionsExclude)
  }

  // TODO this step may use multiple cores - need to add numCores option to each local / uger / google - currently set to multiprocessing.cpu_count()
  google {
    hail"""$pyHailFilter
      --vds-in ${Google.harmRefVds}
      --regions-exclude ${Google.regionsExclude}
      --variant-qc-out ${Google.harmRefFiltVariantQc}
      --variants-prunedin-out ${Google.harmRefFiltVariantsPrunedIn}
      --filt-vds-out ${Google.harmRefFiltVds}
      --filt-plink-out ${Google.harmRefFiltPrefix}
      --filt-pruned-vds-out ${Google.harmRefFiltPrunedVds}
      --filt-pruned-plink-out ${Google.harmRefFiltPrunedPrefix}"""
      .in(Google.harmRefVds, Google.regionsExclude)
      .out(((Google.harmRefFilt :+ Google.harmRefFiltVds) ++ (Google.harmRefFiltPruned :+ Google.harmRefFiltPrunedVds)) :+ Google.harmRefFiltVariantQc :+ Google.harmRefFiltVariantsPrunedIn)
  }

  /**
   * Kinship Step
   *  Description: Calculate kinship to identify duplicates and any samples exhibiting abnormal (excessive) sharing
   *  Requires: King, R, $rCalcKinshipSampleSharing
   *  Input: $harmRefFiltPruned
   *  Output: ${id}.kinshipTMP.dat, ${id}.kinshipTMP.ped, ${id}.kinship.kin, ${id}.kinship.kin0, ${id}.kinship.kin0.related, ${id}.kinship.sharing_counts.txt
   *  Notes:
   *     King is preferred to Plink or Hail based IBD calcs due to robust algorithm handling of population stratification. This step should be followed by a visual inspection for duplicates or excessive sharing
   * King only writes the '.kin0' file if families are found, so a bash script is used to write an empty file in that case
   */

  local {
    googleCopy(Google.harmRefFiltPruned, Local.harmRefFiltPruned)

    cmd"""$shKing $binKing ${Local.harmRefFiltPrunedPrefix}.bed ${Local.kinPrefix} ${Local.kinLog} ${Local.kinKin0} ${Local.kinKin0Related}"""
    .in(Local.harmRefFiltPruned)
    .out(Local.kinLog, Local.kinKin, Local.kinTmpDat, Local.kinTmpPed, Local.kinKin0, Local.kinKin0Related)

    cmd"""$binR --vanilla --args ${Local.kinKin0Related} ${Local.kinFamsizes} < ${rCalcKinshipFamSizes}"""
    .in(Local.kinKin0Related)
    .out(Local.kinFamsizes)
  }

  /**
    * Ancestry PCA Step
    *  Description: Calculate PCs combined with 1KG Phase 3 Purcell 5k data
    *  Requires: Hail, R, $rPlotAncestryPca
    *  Input: $harmRefVds, $kgVds, $kgV3purcell5kAlleleFreqs
    *  Output: ${id}.ancestry.pca.log, ${id}.ancestry.pca.scores.tsv, ${id}.ancestry.pca.loadings.tsv, .${id}.ancestry.pca.scores.tsv.crc,
    *     ${id}.ancestry.pca.loadings.tsv.crc ${id}.ancestry.pca.scores.plots.pdf
    *  Notes:
    *     To perform ancestry inference and clustering with 1KG data, we must combine on common variants with reference data (clustering does not work when only using PCA loadings and projecting)
    */

  local {
    googleCopy(Local.kgPurcellVcf, Google.kgPurcellVcf)
    googleCopy(Local.kgSample, Google.kgSample)
  }

  google {
    hail"""$pyHailAncestryPcaMerge1kg
      --vds-in ${Google.harmRefVds}
      --kg-vcf-in ${Google.kgPurcellVcf}
      --kg-sample ${Google.kgSample}
      --plink-out ${Google.harmRef1kgPrefix}"""
      .in(Google.harmRefVds, Google.kgPurcellVcf, Google.kgSample)
      .out(Google.harmRef1kg)
  }

  local {
    googleCopy(Google.harmRef1kg, Local.harmRef1kg)

    cmd"""$binRscript --vanilla --verbose
      $rPcair
      --plink-in ${Local.harmRef1kgPrefix}
      --gds-out ${Local.harmRef1kgGds}
      --scores ${Local.ancestryPcaScores}
      --id $id
      --update-pop $kgSampleId $kgSamplePop ${Local.kgSample}
      --update-group $kgSampleId $kgSampleGroup ${Local.kgSample}
      > ${Local.ancestryPcaLog}"""
      .in(Local.harmRef1kg :+ Local.kgSample)
      .out(Local.harmRef1kgGds, Local.ancestryPcaLog, Local.ancestryPcaScores)
      .using("R-3.4")

    cmd"""$binR --vanilla --args $id ${Local.ancestryPcaScores} ${Local.ancestryPcaScoresPlots} < $rPlotAncestryPca"""
    .in(Local.ancestryPcaScores)
    .out(Local.ancestryPcaScoresPlots)

    /**
     * Ancestry Cluster Step
     *  Description: Cluster with 1KG samples using Gaussian Mixture Modeling and infer ancestry
     *  Requires: Hail, R, $rPlotAncestryCluster, $id, $phenoId, $phenoSrRace
     *  Input: ${id}.ancestry.pca.scores.tsv, $pheno
     *  Output: ${id}.ancestry.fet.1, ${id}.ancestry.temp.clu.1, ${id}.ancestry.clu.1, ${id}.ancestry.klg.1, ${id}.ancestry.cluster_plots.pdf,
     *     ${id}.ancestry.cluster_xtabs, ${id}.ancestry.cluster_plots.centers.pdf, ${id}.ancestry.clusters_assigned, ${id}.ancestry
     *  Notes:
     *     ${id}.ancestry contains the final inferred ancestry for each sample, including OUTLIERS
     *     This file may be updated after reconciling with other arrays
     */

    cmd"""(echo 3; sed '1d' ${Local.ancestryPcaScores} | cut -f4-6 | sed 's/\t/ /g') > ${Local.ancestryClusterFet}"""
    .in(Local.ancestryPcaScores)
    .out(Local.ancestryClusterFet)

    cmd"""$binKlustakwik ${Local.ancestryClusterPrefix} 1 -UseFeatures 111 -UseDistributional 0 > ${Local.ancestryClusterLog}"""
    .in(Local.ancestryClusterFet)
    .out(Local.ancestryClusterClu, Local.ancestryClusterKlg, Local.ancestryClusterLog)

    cmd"""$binR --vanilla --args ${Local.ancestryPcaScores} ${Local.ancestryClusterClu} ${Local.pheno} $id $phenoId $phenoSrRace
      ${Local.ancestryClusterPlots} ${Local.ancestryClusterXtabs} ${Local.ancestryClusterPlotsCenters}
      ${Local.ancestryClusterGroups} ${Local.ancestryInferred}
      ${Local.ancestryClusterPlotsNo1kg} < $rPlotAncestryCluster"""
      .in(Local.ancestryPcaScores, Local.ancestryClusterClu, Local.pheno)
      .out(Local.ancestryClusterPlots, Local.ancestryClusterXtabs, Local.ancestryClusterPlotsCenters, Local.ancestryClusterGroups, Local.ancestryInferred, Local.ancestryClusterPlotsNo1kg)
  }

  /**
   * PCA Step
   *  Description: Calculate PCs for all non-outlier samples combined (to be used for adjustment during sample outlier removal)
   *  Requires: Hail
   *  Input: $harmRefFiltPrunedVds, $ancestryInferred
   *  Output: ${id}.pca.log, ${id}.pca.scores.tsv, ${id}.pca.loadings.tsv, .${id}.pca.scores.tsv.crc,
   *     ${id}.pca.loadings.tsv.crc
   *  Notes:
   */

  local {
    cmd"""awk '{if($$2 == "OUTLIERS") print $$1}' ${Local.ancestryInferred} > ${Local.ancestryOutliers}""".in(Local.ancestryInferred).out(Local.ancestryOutliers)

    cmd"""$binRscript --vanilla --verbose
      $rPcair
      --plink-in ${Local.harmRefFiltPrunedPrefix}
      --gds-out ${Local.harmRefFiltPrunedPcaGds}
      --exclude ${Local.ancestryOutliers}
      --ancestry ${Local.ancestryInferred}
      --id $id
      --scores ${Local.pcaScores}
      > ${Local.pcaLog}"""
      .in(Local.harmRefFiltPruned :+ Local.ancestryInferred :+ Local.ancestryOutliers)
      .out(Local.harmRefFiltPrunedPcaGds, Local.pcaLog, Local.pcaScores)
      .using("R-3.4")
  }

  /**
   * Sample QC Stats Calculation Step
   *  Description: Calculate sexcheck and sample by variant statistics for all samples
   *  Requires: Hail, R
   *  Input: $harmRefFiltVds, $ancestryInferred, $pcaScores
   *  Output: ${id}.sampleqc.log, ${id}.sampleqc.sexcheck.tsv, ${id}.sampleqc.stats.tsv, ${id}.sampleqc.sexcheck.problems.tsv,
   *     ${id}.sampleqc.stats.adj.tsv, ${id}.sampleqc.stats.adj.corr.pdf, ${id}.sampleqc.stats.adj.pca.loadings.tsv, ${id}.sampleqc.stats.adj.pcs.pdf,
   *     ${id}.sampleqc.stats.adj.pca.scores.tsv
   * Notes:
   */

  local {
    googleCopy(Local.ancestryInferred, Google.ancestryInferred)
    googleCopy(Local.ancestryInferred, Google.ancestryInferred)
    googleCopy(Local.pheno, Google.pheno)
  }

  google {
    hail"""$pyHailSexcheck
      --vds-in ${Google.harmRefVds}
      --regions-exclude ${Google.regionsExclude}
      --pheno-in ${Google.pheno}
      --id-col $phenoId
      --sex-col $phenoSrSex
      --sexcheck-out ${Google.sampleqcSexcheck}
      --sexcheck-problems-out ${Google.sampleqcSexcheckProblems}"""
      .in(Google.pheno, Google.harmRefVds, Google.regionsExclude)
      .out(Google.sampleqcSexcheck, Google.sampleqcSexcheckProblems)

    hail"""$pyHailSampleqc
      --vds-in ${Google.harmRefFiltPrunedVds}
      --clusters-in ${Google.ancestryInferred}
      --qc-out ${Google.sampleqcStats}"""
      .in(Google.harmRefFiltPrunedVds, Google.ancestryInferred)
      .out(Google.sampleqcStats)
  }

  local {
    googleCopy(Google.sampleqcStats, Local.sampleqcStats)
  }

  uger {
    cmd"""$binR --vanilla --args ${Local.sampleqcStats} ${Local.pcaScores} ${Local.sampleqcStatsAdj} < $rCalcIstatsAdj"""
    .in(Local.sampleqcStats, Local.pcaScores)
    .out(Local.sampleqcStatsAdj)

    cmd"""$binR --vanilla --args ${Local.sampleqcStatsAdj} ${Local.sampleqcStatsAdjCorrPlots} ${Local.sampleqcStatsAdjPcaLoadings} ${Local.sampleqcStatsAdjPcaScoresPlots} ${Local.sampleqcStatsAdjPcaScores} < $rIstatsAdjPca"""
    .in(Local.sampleqcStatsAdj)
    .out(Local.sampleqcStatsAdjCorrPlots, Local.sampleqcStatsAdjPcaLoadings, Local.sampleqcStatsAdjPcaScoresPlots, Local.sampleqcStatsAdjPcaScores)
  }

  /**
   * Sample QC PCA Clustering Step
   *  Description: Cluster PCs of adjusted sample QC metrics
   *  Requires: Klustakwik, R
   *  Input: $sampleqcStatsAdjPcaScores, $sampleqcStatsAdj
   *  Output: ${id}.sampleqc.stats.adj.fet.1, ${id}.sampleqc.stats.adj.clu.1, ${id}.sampleqc.stats.adj.temp.clu.1, ${id}.sampleqc.stats.adj.klg.1,
   *     ${id}.sampleqc.stats.adj.pca.outliers.tsv, ${id}.sampleqc.stats.adj.pca.clusters.plot.pdf, ${id}.sampleqc.stats.adj.pca.clusters.xtab,
   *     ${id}.sampleqc.stats.adj.stripchart.pdf
   * Notes:
   */

  uger {
    cmd"""N=$$(head -1 ${Local.sampleqcStatsAdjPcaScores} | wc | awk '{print $$2-1}');
      echo $$N > ${Local.sampleqcStatsAdjClusterKlustakwikStores.fet};
      sed '1d' ${Local.sampleqcStatsAdjPcaScores} | cut -f2- | sed 's/\t/ /g' >> ${Local.sampleqcStatsAdjClusterKlustakwikStores.fet};
      FEATURES=1; for i in $$(seq 2 $$n); do FEATURES=$${FEATURES}1; done;
      $binKlustakwik ${Local.sampleqcStatsAdjClusterKlustakwikStores.base} 1 -UseFeatures $$FEATURES -UseDistributional 0 >
      ${Local.sampleqcStatsAdjClusterKlustakwikStores.klustakwikLog}"""
      .in(Local.sampleqcStatsAdjClusterKlustakwikStores.inputs + Local.sampleqcStatsAdjPcaScores)
      .out(Local.sampleqcStatsAdjClusterKlustakwikStores.outputs)

    cmd"""$binR --vanilla --args ${Local.sampleqcStatsAdjPcaScores} ${Local.sampleqcStatsAdjClusterKlustakwikStores.clu}
      ${Local.sampleqcStatsAdjClusterOutliers} ${Local.sampleqcStatsAdjClusterPlots}
      ${Local.sampleqcStatsAdjClusterXtabs} $id < $rIstatsPcsGmmClusterPlot"""
      .in(Local.sampleqcStatsAdjPcaScores,  Local.sampleqcStatsAdjClusterKlustakwikStores.clu)
      .out(Local.sampleqcStatsAdjClusterOutliers, Local.sampleqcStatsAdjClusterPlots, Local.sampleqcStatsAdjClusterXtabs)

  }

  /**
   * Sample QC Individual Stats Clustering Step
   *  Description: Cluster PCs of adjusted sample QC metrics
   *  Requires: Klustakwik, R
   *  Input: $sampleqcStats, $sampleqcStatsAdj, $sampleqcStatsAdj, $sampleqcStatsAdjClusterOutliers, $ancestryInferred
   *  Output: ${id}.sampleqc.stats.adj.*.fet.1, ${id}.sampleqc.stats.adj.*.clu.1, ${id}.sampleqc.stats.adj.*.temp.clu.1, ${id}.sampleqc.stats.adj.*.klg.1,
   *     ${id}.sampleqc.stats.adj.*.klustakwik.log, ${id}.sampleqc.stats.adj.individual.boxplot.pdf, ${id}.sampleqc.stats.adj.individual.discreteness,
   *     ${id}.sampleqc.stats.adj.individual.outliers.table, ${id}.sampleqc.stats.adj.individual.outliers.remove, ${id}.sampleqc.stats.adj.individual.stripchart.pdf
   * Notes:
   */
  
  val sampleQcKlustakwikStores: Seq[KlustakwikStores] = {
    uger {
      sampleQcMetrics.map { metricId =>
        val stores = KlustakwikStores(s"${Local.sampleqcStatsAdjPrefix}.${metricId}")
  
        cmd"""echo 1 > ${stores.fet};
          metricIdx=`head -1 ${Local.sampleqcStatsAdj} | tr '\t' '\n' | awk '{print NR" "$$0}' | grep -w ${metricId} | awk '{print $$1}'`;
          sed '1d' ${Local.sampleqcStatsAdj} | awk -v col=$${metricIdx} '{print $$col}' >> ${stores.fet}"""
          .in(Local.sampleqcStatsAdj)
          .out(stores.fet)
  
        cmd"""$binKlustakwik ${stores.base} 1 -UseFeatures 1 -UseDistributional 0 > ${stores.klustakwikLog}"""
        .in(stores.inputs)
        .out(stores.outputs)
  
        stores
      }
    }
  }
  
  local {
    cmd"""$binR --vanilla --args
      ${sampleQcMetrics.mkString(",")}
      ${Local.sampleqcStats}
      ${Local.sampleqcStatsAdj}
      ${Local.sampleqcStatsAdjClusterOutliers}
      ${Local.sampleqcStatsAdjIndBoxplots}
      ${Local.sampleqcStatsAdjIndDiscreteness}
      ${Local.sampleqcStatsAdjOutliersTable}
      ${Local.sampleqcStatsAdjStripchart}
      ${Local.ancestryInferred}
      < $rIstatsAdjGmmPlotMetrics"""
      .in(sampleQcKlustakwikStores.map(_.clu) :+ Local.sampleqcStats :+ Local.sampleqcStatsAdj :+ Local.ancestryInferred :+ Local.sampleqcStatsAdjClusterOutliers)
      .out(Local.sampleqcStatsAdjIndBoxplots, Local.sampleqcStatsAdjIndDiscreteness, Local.sampleqcStatsAdjOutliersTable, Local.sampleqcStatsAdjStripchart)
  }
  
  /**
   * Compile Sample Exclusions Step
   */
  
  local {
    googleCopy(Local.kinKin0Related, Google.kinKin0Related)
    googleCopy(Local.kinFamsizes, Google.kinFamsizes)
    googleCopy(Local.sampleqcStatsAdjOutliersTable, Google.sampleqcStatsAdjOutliersTable)
  }

  google {
    hail"""$pyCompileExclusions
      --ancestry-inferred ${Google.ancestryInferred}
      --kinship-related ${Google.kinKin0Related}
      --kinship-famsizes ${Google.kinFamsizes}
      --sampleqc-outliers ${Google.sampleqcStatsAdjOutliersTable}
      --sexcheck-problems ${Google.sampleqcSexcheckProblems}
      --ancestry-keep ${ancestryKeep.mkString(",")}
      --duplicates-keep ${duplicatesKeep.mkString(",")}
      --famsize-keep ${famsizeKeep.mkString(",")}
      --sampleqc-keep ${sampleqcKeep.mkString(",")}
      --sexcheck-keep ${sexcheckKeep.mkString(",")}
      --out ${Google.finalSampleExclusions}"""
      .in(Google.ancestryInferred, Google.kinKin0Related, Google.kinFamsizes, Google.sampleqcStatsAdjOutliersTable, Google.sampleqcSexcheckProblems)
      .out(Google.finalSampleExclusions)
  }
  
  /**
   * Filter Clean Step
   * filter variants and generate final clean dataset
   */

  google {
    hail"""$pyHailFilterFinal
      --vds-in ${Google.harmRefVds}
      --ancestry-in ${Local.ancestryInferred}
      --sexcheck-in ${Google.sampleqcSexcheck}
      --pheno-in ${Google.pheno}
      --case-ctrl-col $phenoStatus
      --samples-remove ${Google.finalSampleExclusions}
      --plink-out ${Local.cleanPrefix}"""
      .in(Google.harmRefVds, Google.ancestryInferred, Google.sampleqcSexcheck, Google.pheno, Google.finalSampleExclusions)
      .out(Google.clean)
  }

  local {
    googleCopy(Google.clean, Local.clean)

    cmd"""$binRscript --vanilla --verbose
      $rPcair
      --plink-in ${Local.cleanPrefix}
      --gds-out ${Local.cleanPcaGds}
      --rdata ${Local.cleanPcaScoresRdata}
      --ancestry ${Local.ancestryInferred}
      --id $id
      --scores ${Local.cleanPcaScores}
      > ${Local.cleanPcaLog} """
      .in(Local.clean :+ Local.ancestryInferred).out(Local.cleanPcaGds, Local.cleanPcaScores, Local.cleanPcaScoresRdata, Local.cleanPcaLog)
      .using("R-3.4")
  
    cmd"""$binRscript --vanilla --verbose
      $rPcrelate
      --gds-in ${Local.cleanPcaGds}
      --rdata-in ${Local.cleanPcaScoresRdata}
      --ibd-out ${Local.cleanRelateScores}
      > ${Local.cleanRelateLog}"""
      .in(Local.cleanPcaGds, Local.cleanPcaScoresRdata)
      .out(Local.cleanRelateScores, Local.cleanRelateLog)
      .using("R-3.4")
  }
}
