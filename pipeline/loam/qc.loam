import binaries._
import cloud_helpers._
import config._
import scripts._
import store_helpers._

import loamstream.conf.DataConfig
import loamstream.googlecloud.HailSupport._
import loamstream.model.Store

val nChr = endChr - startChr + 1
val kgSampleId = dataConfig.getStr("kgSampleId")
val kgSamplePop = dataConfig.getStr("kgSamplePop")
val kgSampleGroup = dataConfig.getStr("kgSampleGroup")
val kgVcfBaseWild = dataConfig.getStr("kgVcfBaseWild")
val kgLegendWild = dataConfig.getStr("kgLegendWild")
val humanReferenceWild = dataConfig.getStr("humanReferenceWild")
val phenoId = dataConfig.getStr("phenoId")
val phenoSrSex = dataConfig.getStr("phenoSrSex")
val phenoSrRace = dataConfig.getStr("phenoSrRace")
val phenoStatus = dataConfig.getStr("phenoStatus")
val nArrays = dataConfig.getObjList("arrays").size
val nMetrics = sampleQcMetrics.size

object Input {

  object Local {
    val kgPurcellVcf = store[VCF].at(path(dataConfig.getStr("kgPurcellVcf"))).asInput
    val kgSample = store[TXT].at(path(dataConfig.getStr("kgSample"))).asInput
    val regionsExclude = store[TXT].at(path(dataConfig.getStr("regionsExclude"))).asInput
    val pheno = store[TXT].at(path(dataConfig.getStr("pheno"))).asInput
  }

  object Google {
    val kgPurcellVcf = store[VCF].at(uri(dataConfig.getStr("kgPurcellVcfGoogle"))).asInput
    val kgSample = store[TXT].at(uri(dataConfig.getStr("kgSampleGoogle"))).asInput
    val regionsExclude = store[TXT].at(uri(dataConfig.getStr("regionsExcludeGoogle"))).asInput
    val pheno = store[TXT].at(googleOutDir / s"${Input.Local.pheno}".split("/").last)
  }

}

object Params {

  final case class Chr(
    rawChrName: Path,
    rawChr: Seq[Store[TXT]],
    harmKgChrName: Path,
    harmKgHuRefChrName: Path,
    harmKgChr: Seq[Store[TXT]],
    harmKgHuRefChr: Seq[Store[TXT]],
    kgVcfChr: Store[VCF],
    kgLegendChr: Store[TXT],
    humanReference: Store[TXT],
    harmKgChrRemove: Store[TXT],
    harmKgChrNonKgFlip: Store[TXT],
    harmKgChrForceA1: Store[TXT],
    harmKgChrVarIdUpdate: Store[TXT],
    harmKgChrVarSnpLog: Store[TXT],
    harmMergeLine: String)

  final case class Metric(
    sampleqcStatsAdjIndMetric: String,
    sampleqcStatsAdjIndClusterName: Path,
    sampleqcStatsAdjIndClusterFet: Store[TXT],
    sampleqcStatsAdjIndClusterClu: Store[TXT],
    sampleqcStatsAdjIndClusterKlg: Store[TXT],
    sampleqcStatsAdjIndClusterKlustakwikLog: Store[TXT])

  final case class Arr(
    arrayId: String,
    harmName: Path,
    harm: Seq[Store[TXT]],
    harmRefName: Path,
    harmRefVcf: Store[VCF],
    harmRefVcfCloud: Store[VCF],
    harmRefVcfTbi: Store[TXT],
    harmRefVcfTbiCloud: Store[VCF],
    harmRefVdsCloud: Store[VCF],
    harmRefFilt: Seq[Store[TXT]],
    harmRefFiltNameCloud: URI,
    harmRefFiltCloud: Seq[Store[TXT]],
    harmRefFiltVdsCloud: Store[VCF],
    harmRefFiltVariantQcCloud: Store[TXT],
    harmRefFiltVariantsPrunedInCloud: Store[TXT],
    harmRefFiltPrunedName: Path,
    harmRefFiltPruned: Seq[Store[TXT]],
    harmRefFiltPrunedNameCloud: URI,
    harmRefFiltPrunedCloud: Seq[Store[TXT]],
    harmRefFiltPrunedVds: Store[VCF],
    harmRefFiltPrunedVdsCloud: Store[VCF],
    paramsByChr: Seq[(Int, Chr)],
    paramsByMetric: Seq[(Int, Metric)],
    paramsSortedByChr: Seq[Chr],
    paramsSortedByMetric: Seq[Metric],
    harmMergeLines: Seq[String],
    harmMergeList: Store[TXT],
    sampleqcStatsAdjIndClusterCluList: Seq[String],
    harmForceA2: Store[TXT],
    kinName: Path,
    kinLog: Store[TXT],
    kinTmpDat: Store[TXT],
    kinTmpPed: Store[TXT],
    kinKin: Store[TXT],
    kinKin0: Store[TXT],
    kinKin0Related: Store[TXT],
    kinFamsizes: Store[TXT],
    ancestryPcaName: Path,
    harmRef1kgName: Path,
    harmRef1kg: Seq[Store[TXT]],
    harmRef1kgGds: Store[TXT],
    ancestryPcaLog: Store[TXT],
    ancestryPcaScores: Store[TXT],
    ancestryPcaScoresPlots: Store[TXT],
    harmRef1kgNameCloud: URI,
    harmRef1kgCloud: Seq[Store[TXT]],
    ancestryClusterName: Path,
    ancestryClusterLog: Store[TXT],
    ancestryClusterFet: Store[TXT],
    ancestryClusterClu: Store[TXT],
    ancestryClusterKlg: Store[TXT],
    ancestryClusterPlots: Store[TXT],
    ancestryClusterPlotsCenters: Store[TXT],
    ancestryClusterPlotsNo1kg: Store[TXT],
    ancestryClusterXtabs: Store[TXT],
    ancestryClusterGroups: Store[TXT],
    ancestryInferred: Store[TXT],
    harmRefFiltPrunedPcaGds: Store[TXT],
    pcaLog: Store[TXT],
    pcaScores: Store[TXT],
    pcaLoadings: Store[TXT],
    sampleqcStats: Store[TXT],
    sampleqcStatsAdj: Store[TXT],
    sampleqcStatsAdjCorrPlots: Store[TXT],
    sampleqcStatsAdjPcaLoadings: Store[TXT],
    sampleqcStatsAdjPcaScoresPlots: Store[TXT],
    sampleqcStatsAdjPcaScores: Store[TXT],
	sampleqcSexcheck: Store[TXT],
    sampleqcSexcheckProblems: Store[TXT],
    sampleqcSexcheckCloud: Store[TXT],
    sampleqcSexcheckProblemsCloud: Store[TXT],
    sampleqcStatsCloud: Store[TXT],
    sampleqcStatsAdjClusterFet: Store[TXT],
    sampleqcStatsAdjClusterClu: Store[TXT],
    sampleqcStatsAdjClusterKlg: Store[TXT],
    sampleqcStatsAdjClusterKlustakwikLog: Store[TXT],
    sampleqcStatsAdjClusterOutliers: Store[TXT],
    sampleqcStatsAdjClusterPlots: Store[TXT],
    sampleqcStatsAdjClusterXtabs: Store[TXT],
    sampleqcStatsAdjClusterStripchart: Store[TXT],
    sampleqcStatsAdjClusterName: Path,
    sampleqcStatsAdjIndBoxplots: Store[TXT],
    sampleqcStatsAdjIndDiscreteness: Store[TXT],
    sampleqcStatsAdjOutliersTable: Store[TXT],
    sampleqcStatsAdjStripchart: Store[TXT],
    finalSampleExclusions: Store[TXT],
    finalSampleExclusionsCloud: Store[TXT],
    variantqcStats: Store[TXT],
    variantqcStatsCloud: Store[TXT],
    finalVariantExclusions: Store[TXT],
    finalVariantExclusionsCloud: Store[TXT],
    clean: Seq[Store[TXT]],
    cleanNameCloud: URI,
    cleanCloud: Seq[Store[TXT]],
    cleanVcf: Store[VCF],
	cleanVds: Store[VCF],
    cleanVcfCloud: Store[VCF],
    cleanVcfTbi: Store[TXT],
    cleanVdsCloud: Store[VCF])

  val paramsByArr: Seq[(Int, Arr)] = (0 until nArrays).map { arr =>

    val arrayId = dataConfig.getObjList("arrays")(arr).getStr("arrayId")

    val paramsByChr = (startChr to endChr).map { chr =>

      val rawChrBase = s"${projectId}.${arrayId}.chr$chr"
      val harmKgChrBase = s"${rawChrBase}.harmkg"
      val harmKgChrName = localOutDir / harmKgChrBase
      val harmKgHuRefChrName = localOutDir / s"${harmKgChrBase}.huref"
      val rawChrName = localOutDir / rawChrBase
      val kgVcfChr = store[VCF].at(
          kgVcfBaseWild
            .replace("[CHROMOSOME]", s"$chr")
            .replace("23.phase3_shapeit2_mvncall_integrated_v5a",
                     "X.phase3_shapeit2_mvncall_integrated_v1b") + ".vcf.gz").asInput
      val kgLegendChr = store[TXT].at(
          kgLegendWild
            .replace("[CHROMOSOME]", s"$chr")
            .replace("23",
                     "X")).asInput
      val humanReference = store[TXT].at(
          humanReferenceWild
            .replace("[CHROMOSOME]", s"$chr")
            .replace("23",
                     "X")).asInput

      chr -> Chr(
        rawChrName = rawChrName,
        rawChr = bedBimFam(rawChrName),
        harmKgChrName = harmKgChrName,
        harmKgHuRefChrName = harmKgHuRefChrName,
        harmKgChr = bedBimFam(harmKgChrName),
        harmKgHuRefChr = bedBimFam(harmKgHuRefChrName),
        kgVcfChr = kgVcfChr,
        kgLegendChr = kgLegendChr,
        humanReference = humanReference,
        harmKgChrRemove = store[TXT].at(localOutDir / s"${harmKgChrBase}.remove"),
        harmKgChrNonKgFlip = store[TXT].at(localOutDir / s"${harmKgChrBase}.nonkg.flip"),
        harmKgChrForceA1 = store[TXT].at(localOutDir / s"${harmKgChrBase}.force_a1"),
        harmKgChrVarIdUpdate = store[TXT].at(localOutDir / s"${harmKgChrBase}_idUpdates.txt"),
        harmKgChrVarSnpLog = store[TXT].at(localOutDir / s"${harmKgChrBase}_snpLog.log"),
        harmMergeLine = s"${localOutDir}/${harmKgChrBase}")
    }

    val sampleqcBase = s"${projectId}.${arrayId}.sampleqc"
    val sampleqcStatsAdjBase = s"${sampleqcBase}.stats.adj"

    val paramsByMetric: Seq[(Int, Metric)] = (0 until nMetrics).map { metricIdx =>

      val sampleqcStatsAdjIndMetric = sampleQcMetrics(metricIdx)
      val sampleqcStatsAdjIndClusterBase = s"${sampleqcStatsAdjBase}.${sampleqcStatsAdjIndMetric}"

      metricIdx -> Metric(
        sampleqcStatsAdjIndMetric = sampleqcStatsAdjIndMetric,
        sampleqcStatsAdjIndClusterName = localOutDir / sampleqcStatsAdjIndClusterBase,
        sampleqcStatsAdjIndClusterFet = store[TXT].at(localOutDir / s"${sampleqcStatsAdjIndClusterBase}.fet.1"),
        sampleqcStatsAdjIndClusterClu = store[TXT].at(localOutDir / s"${sampleqcStatsAdjIndClusterBase}.clu.1"),
        sampleqcStatsAdjIndClusterKlg = store[TXT].at(localOutDir / s"${sampleqcStatsAdjIndClusterBase}.klg.1"),
        sampleqcStatsAdjIndClusterKlustakwikLog = store[TXT].at(localOutDir / s"${sampleqcStatsAdjIndClusterBase}.klustakwik.log"))
    }

    val harmBase = s"${projectId}.${arrayId}.harm"
    val harmName = localOutDir / harmBase
    val harmRefBase = s"${harmBase}.ref"
    val harmRefFiltBase = s"${harmRefBase}.filt"
    val harmRefFiltPrunedBase = s"${harmRefFiltBase}.pruned"
    val harmRefFiltPrunedName = localOutDir / harmRefFiltPrunedBase
    val harmRefFiltPrunedNameCloud = googleOutDir / s"${harmRefFiltBase}.pruned"
    val harmRefFiltName = localOutDir / harmRefFiltBase
    val kinBase = s"${projectId}.${arrayId}.kinship"
    val harmRef1kgBase = s"${projectId}.${arrayId}.harm.ref.1kg"
    val harmRef1kgName = localOutDir / harmRef1kgBase
    val harmRef1kgNameCloud = googleOutDir / harmRef1kgBase
    val pcaBase = s"${projectId}.${arrayId}.pca"
    val ancestryBase = s"${projectId}.${arrayId}.ancestry"
    val ancestryPcaBase = s"${ancestryBase}.pca"
    val ancestryClusterBase = s"${projectId}.${arrayId}.ancestry.cluster"
    val sampleqcSexcheckBase = s"${projectId}.${arrayId}.sampleqc.sexcheck"
    val sampleqcStatsAdjOutliersTableBase = s"${sampleqcBase}.outliers.tsv"
    val finalBase = s"${projectId}.${arrayId}.final"
    val cleanBase = s"${projectId}.${arrayId}.clean"
    val cleanName = localOutDir / cleanBase
    val cleanNameCloud = googleOutDir / cleanBase
    val sampleqcStatsAdjClusterBase = s"${sampleqcBase}.stats.adj.cluster"
    val sampleqcStatsAdjClusterName = localOutDir / sampleqcStatsAdjClusterBase
    val variantqcBase = s"${projectId}.${arrayId}.variantqc"

    val (_, paramsSortedByChr) = paramsByChr.unzip
    val harmMergeLines = paramsSortedByChr.map(_.harmKgHuRefChrName.toString)

    val (_, paramsSortedByMetric) = paramsByMetric.unzip
    val sampleqcStatsAdjIndClusterCluList = paramsSortedByMetric.map(_.sampleqcStatsAdjIndClusterClu.toString)

    arr -> Arr(
    arrayId = arrayId,
    paramsByChr = paramsByChr,
    paramsByMetric = paramsByMetric,
    harmName = harmName,
    harm = bedBimFam(harmName),
    harmRefName = localOutDir / harmRefBase,
    harmRefVcf = store[VCF].at(localOutDir / s"${harmRefBase}.vcf.gz"),
    harmRefVcfCloud = store[VCF].at(googleOutDir / s"${harmRefBase}.vcf.gz"),
    harmRefVcfTbi = store[TXT].at(localOutDir / s"${harmRefBase}.vcf.gz.tbi"),
    harmRefVcfTbiCloud = store[VCF].at(googleOutDir / s"${harmRefBase}.vcf.gz.tbi"),
    harmRefVdsCloud = store[VCF].at(googleOutDir / s"${harmRefBase}.vds"),
    paramsSortedByChr = paramsSortedByChr,
    paramsSortedByMetric = paramsSortedByMetric,
    harmMergeLines = harmMergeLines,
    sampleqcStatsAdjIndClusterCluList = sampleqcStatsAdjIndClusterCluList,
    harmMergeList = store[TXT].at(localOutDir / s"${harmBase}.merge.txt"),
    harmForceA2 = store[TXT].at(localOutDir / s"${harmBase}.force_a2.txt"),
    harmRefFilt = bedBimFam(harmRefFiltName),
    harmRefFiltNameCloud = googleOutDir / harmRefFiltBase,
    harmRefFiltCloud = bedBimFam(googleOutDir / harmRefFiltBase),
    harmRefFiltVdsCloud = store[VCF].at(googleOutDir / s"${harmRefFiltBase}.vds"),
    harmRefFiltVariantQcCloud = store[TXT].at(googleOutDir / s"${harmRefFiltBase}.variantqc.tsv"),
    harmRefFiltVariantsPrunedInCloud = store[TXT].at(googleOutDir / s"${harmRefFiltPrunedBase}.in"),
    harmRefFiltPrunedName = harmRefFiltPrunedName,
    harmRefFiltPruned = bedBimFam(harmRefFiltPrunedName),
    harmRefFiltPrunedNameCloud = harmRefFiltPrunedNameCloud,
    harmRefFiltPrunedCloud = bedBimFam(harmRefFiltPrunedNameCloud),
    harmRefFiltPrunedVds = store[VCF].at(localOutDir / s"${harmRefFiltPrunedBase}.vds"),
    harmRefFiltPrunedVdsCloud = store[VCF].at(googleOutDir / s"${harmRefFiltPrunedBase}.vds"),
    kinName = localOutDir / kinBase,
    kinLog = store[TXT].at(localOutDir / s"${kinBase}.log"),
    kinTmpDat = store[TXT].at(localOutDir / s"${kinBase}TMP.dat"),
    kinTmpPed = store[TXT].at(localOutDir / s"${kinBase}.TMP.ped"),
    kinKin = store[TXT].at(localOutDir / s"${kinBase}.kin"),
    kinKin0 = store[TXT].at(localOutDir / s"${kinBase}.kin0"),
    kinKin0Related = store[TXT].at(localOutDir / s"${kinBase}.kin0.related"),
    kinFamsizes = store[TXT].at(localOutDir / s"${kinBase}.famsizes.tsv"),
    ancestryPcaName = localOutDir / ancestryPcaBase,
    harmRef1kgName = harmRef1kgName,
    harmRef1kg = bedBimFam(harmRef1kgName),
    harmRef1kgGds = store[TXT].at(localOutDir / s"${harmRef1kgBase}.gds"),
    ancestryPcaLog = store[TXT].at(localOutDir / s"${ancestryPcaBase}.log"),
    ancestryPcaScores = store[TXT].at(localOutDir / s"${ancestryPcaBase}.scores.tsv"),
    ancestryPcaScoresPlots = store[TXT].at(localOutDir / s"${ancestryPcaBase}.scores.plots.pdf"),
    harmRef1kgNameCloud = harmRef1kgNameCloud,
    harmRef1kgCloud = bedBimFam(harmRef1kgNameCloud),
    ancestryClusterName = localOutDir / ancestryClusterBase,
    ancestryClusterLog = store[TXT].at(localOutDir / s"${ancestryClusterBase}.log"),
    ancestryClusterFet = store[TXT].at(localOutDir / s"${ancestryClusterBase}.fet.1"),
    ancestryClusterClu = store[TXT].at(localOutDir / s"${ancestryClusterBase}.clu.1"),
    ancestryClusterKlg = store[TXT].at(localOutDir / s"${ancestryClusterBase}.klg.1"),
    ancestryClusterPlots = store[TXT].at(localOutDir / s"${ancestryClusterBase}.plots.pdf"),
    ancestryClusterPlotsCenters = store[TXT].at(localOutDir / s"${ancestryClusterBase}.plots.centers.pdf"),
    ancestryClusterPlotsNo1kg = store[TXT].at(localOutDir / s"${ancestryClusterBase}.plots.no_1kg.pdf"),
    ancestryClusterXtabs = store[TXT].at(localOutDir / s"${ancestryClusterBase}.xtabs"),
    ancestryClusterGroups = store[TXT].at(localOutDir / s"${ancestryClusterBase}.groups.tsv"),
    ancestryInferred = store[TXT].at(localOutDir / s"${ancestryBase}.inferred.tsv"),
    harmRefFiltPrunedPcaGds = store[TXT].at(localOutDir / s"${harmRefFiltPrunedBase}.pca.gds"),
    pcaLog = store[TXT].at(localOutDir / s"${pcaBase}.log"),
    pcaScores = store[TXT].at(localOutDir / s"${pcaBase}.scores.tsv"),
    pcaLoadings = store[TXT].at(localOutDir / s"${pcaBase}.loadings.tsv"),
    sampleqcStats = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.tsv"),
    sampleqcStatsAdj = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.tsv"),
    sampleqcStatsAdjCorrPlots = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.corr.pdf"),
    sampleqcStatsAdjPcaLoadings = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.pca.loadings.tsv"),
    sampleqcStatsAdjPcaScoresPlots = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.pca.plots.pdf"),
    sampleqcStatsAdjPcaScores = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.pca.scores.tsv"),
	sampleqcSexcheck = store[TXT].at(localOutDir / s"${sampleqcSexcheckBase}.tsv"),
    sampleqcSexcheckProblems = store[TXT].at(localOutDir / s"${sampleqcSexcheckBase}.problems.tsv"),
    sampleqcSexcheckCloud = store[TXT].at(googleOutDir / s"${sampleqcSexcheckBase}.tsv"),
    sampleqcSexcheckProblemsCloud = store[TXT].at(googleOutDir / s"${sampleqcSexcheckBase}.problems.tsv"),
    sampleqcStatsCloud = store[TXT].at(googleOutDir / s"${sampleqcBase}.stats.tsv"),
    sampleqcStatsAdjClusterName = sampleqcStatsAdjClusterName,
    sampleqcStatsAdjClusterFet = store[TXT].at(localOutDir / s"${sampleqcStatsAdjClusterBase}.fet.1"),
    sampleqcStatsAdjClusterClu = store[TXT].at(localOutDir / s"${sampleqcStatsAdjClusterBase}.clu.1"),
    sampleqcStatsAdjClusterKlg = store[TXT].at(localOutDir / s"${sampleqcStatsAdjClusterBase}.klg.1"),
    sampleqcStatsAdjClusterKlustakwikLog = store[TXT].at(localOutDir / s"${sampleqcStatsAdjClusterBase}.klustakwik.log"),
    sampleqcStatsAdjClusterOutliers = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.cluster.outliers"),
    sampleqcStatsAdjClusterPlots = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.cluster.plots.pdf"),
    sampleqcStatsAdjClusterXtabs = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.cluster.xtabs"),
    sampleqcStatsAdjClusterStripchart = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.cluster.stripchart.pdf"),
    sampleqcStatsAdjIndBoxplots = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.ind.boxplots.pdf"),
    sampleqcStatsAdjIndDiscreteness = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.ind.discreteness"),
    sampleqcStatsAdjOutliersTable = store[TXT].at(localOutDir / sampleqcStatsAdjOutliersTableBase),
    sampleqcStatsAdjStripchart = store[TXT].at(localOutDir / s"${sampleqcBase}.stats.adj.stripchart.pdf"),
    finalSampleExclusions = store[TXT].at(localOutDir / s"${finalBase}.sample.exclusions"),
    finalSampleExclusionsCloud = store[TXT].at(googleOutDir / s"${finalBase}.sample.exclusions"),
    variantqcStats = store[TXT].at(localOutDir / s"${variantqcBase}.stats.tsv"),
    variantqcStatsCloud = store[TXT].at(googleOutDir / s"${variantqcBase}.stats.tsv"),
	finalVariantExclusions = store[TXT].at(localOutDir / s"${finalBase}.variant.exclusions"),
	finalVariantExclusionsCloud = store[TXT].at(googleOutDir / s"${finalBase}.variant.exclusions"),
    clean = bedBimFam(cleanName),
    cleanNameCloud = cleanNameCloud,
    cleanCloud = bedBimFam(cleanNameCloud),
    cleanVcf = store[VCF].at(localOutDir / s"${cleanBase}.vcf.bgz"),
    cleanVcfTbi = store[TXT].at(localOutDir / s"${cleanBase}.vcf.bgz.tbi"),
    cleanVcfCloud = store[VCF].at(googleOutDir / s"${cleanBase}.vcf.bgz"),
    cleanVds = store[VCF].at(localOutDir / s"${cleanBase}.vds"),
    cleanVdsCloud = store[VCF].at(googleOutDir / s"${cleanBase}.vds"))
    }

}

// Perform qc1
val ancestryInferredStores: Seq[Store[TXT]] = {
  for {
    i <- 0 until nArrays
  } yield { 
    qc1(i)
  }
}

// Reconcile inferred ancestry step
val ancestryInferredPathList = ancestryInferredStores.map(_.path).mkString(",")
val ancestryInferredMerged = store[TXT].at(localOutDir / s"${projectId}.ancestry.inferred.merged.tsv")
val ancestryInferredMergedCloud = store[TXT].at(googleOutDir / s"${projectId}.ancestry.inferred.merged.tsv")
val ancestryInferredMergedOutliers = store[TXT].at(localOutDir / s"${projectId}.ancestry.inferred.merged.outliers.tsv")

local {
  cmd"""$binRscript --vanilla --verbose
    $rAncestryClusterMerge
    --ancestry-in $ancestryInferredPathList
    --out-table ${ancestryInferredMerged}
    --out-outliers ${ancestryInferredMergedOutliers}""".in(ancestryInferredStores).out(ancestryInferredMerged, ancestryInferredMergedOutliers).using("R-3.4")
}

// Perform qc2, adding relevent (inter-array) stores to the map
for {
  i <- 0 until nArrays
} { 
  qc2(i)
}

def qc1(i: Int): Store[TXT] = {

  // Global parameters
  val dataPath = dataConfig.getObjList("arrays")(i).getStr("dataPath")
  val dataType = dataConfig.getObjList("arrays")(i).getStr("dataType")

  // Input Stores
  val data = {
    if (dataType == "vcf") { store[VCF].at(path(dataPath)).asInput }
    else { store[TXT].at(path(s"${dataPath}.bed")).asInput }
  }

  val (_, paramsArr) = Params.paramsByArr(i)

  /**
  * Params Step
  *  Description: Align data strand to 1KG reference. Also, update reference allele and variant ID to match 1KG
  *  Requires: Plink1.9 and, at least, Genotype Harmonizer v1.4.18
  *  Notes:
  *     Could also add --variants and --mafAlign as pipeline options, but for now these are static
  *     To save time, this will be run in parallel by chromosome number
  */
  
  uger {
    for {
      (chr, paramsChr) <- paramsArr.paramsByChr
    } {
      cmd"""$binPlink --$dataType $dataPath --chr $chr --keep-allele-order --make-bed --output-chr MT --out ${paramsChr.rawChrName}"""
        .in(data)
        .out(paramsChr.rawChr)
      
      cmd"""$binGenotypeHarmonizer
      --input ${paramsChr.rawChrName}
      --inputType PLINK_BED
      --output ${paramsChr.harmKgChrName}
      --outputType PLINK_BED
      --ref ${paramsChr.kgVcfChr}
      --refType VCF
      --keep
      --update-id
      --variants 1000
      --mafAlign 0.1
      --update-id
      --update-reference-allele
      --debug"""
        .in(paramsChr.rawChr :+ paramsChr.kgVcfChr)
        .out(paramsChr.harmKgChr :+ paramsChr.harmKgChrVarIdUpdate :+ paramsChr.harmKgChrVarSnpLog)
  
      cmd"""python $pyAlignNon1kgVariants
      --legend ${paramsChr.kgLegendChr}
      --bim ${paramsChr.harmKgChrName}.bim
      --ref ${paramsChr.humanReference}
      --out-remove ${paramsChr.harmKgChrRemove}
      --out-flip ${paramsChr.harmKgChrNonKgFlip}
      --out-force-a1 ${paramsChr.harmKgChrForceA1}"""
        .in(paramsChr.harmKgChr :+ paramsChr.kgLegendChr)
        .out(paramsChr.harmKgChrNonKgFlip, paramsChr.harmKgChrForceA1)
  
      cmd"""$binPlink --bfile ${paramsChr.harmKgChrName} --exclude ${paramsChr.harmKgChrRemove} --flip ${paramsChr.harmKgChrNonKgFlip} --a1-allele ${paramsChr.harmKgChrForceA1} --make-bed --out ${paramsChr.harmKgHuRefChrName}"""
        .in(paramsChr.harmKgChr :+ paramsChr.harmKgChrRemove :+ paramsChr.harmKgChrNonKgFlip :+ paramsChr.harmKgChrForceA1)
        .out(paramsChr.harmKgHuRefChr)
  
    }
  
    val harmMergeLinesConcat: String = paramsArr.harmMergeLines
      .drop(1)
      .mkString("\n") // Exclude first chrom
    
    cmd"""echo "$harmMergeLinesConcat" > ${paramsArr.harmMergeList}"""
      .out(paramsArr.harmMergeList)
    
    cmd"""$binPlink --bfile ${paramsArr.paramsSortedByChr.head.harmKgHuRefChrName} --merge-list ${paramsArr.harmMergeList} --make-bed --keep-allele-order --out ${paramsArr.harmName}"""
      .in(paramsArr.paramsSortedByChr.flatMap(_.harmKgHuRefChr) :+ paramsArr.harmMergeList)
      .out(paramsArr.harm)
    
    cmd"""awk '{print $$2,$$5}' ${paramsArr.harmName}.bim > ${paramsArr.harmForceA2}"""
      .in(paramsArr.harm)
      .out(paramsArr.harmForceA2)
    
    cmd"""$binPlink --bfile ${paramsArr.harmName} --recode vcf-iid bgz --real-ref-alleles --a2-allele ${paramsArr.harmForceA2} --out ${paramsArr.harmRefName}"""
      .in(paramsArr.harm :+ paramsArr.harmForceA2)
      .out(paramsArr.harmRefVcf)
    
    cmd"""$binTabix -f -p vcf ${paramsArr.harmRefVcf}"""
      .in(paramsArr.harmRefVcf)
      .out(paramsArr.harmRefVcfTbi)
  }
  
  /**
   * Load Step
   *  Description: Generate the Hail VDS from VCF file and a sample file containing population and sex information
   *  Requires: Hail
   */
  
  local {
    googleCopy(paramsArr.harmRefVcf, paramsArr.harmRefVcfCloud)
    googleCopy(paramsArr.harmRefVcfTbi, paramsArr.harmRefVcfTbiCloud)
  }
  
  google {
    hail"""$pyHailLoad
      --vcf-in $projectId ${paramsArr.harmRefVcfCloud}
      --vds-out ${paramsArr.harmRefVdsCloud}"""
      .in(paramsArr.harmRefVcfCloud, paramsArr.harmRefVcfTbiCloud)
      .out(paramsArr.harmRefVdsCloud)
  }
  
  /**
   * Filter Step
   *  Description: Generate filtered and filtered/pruned filesets for QC
   *  Requires: Hail
   */

  google {
    hail"""$pyHailFilter
      --vds-in ${paramsArr.harmRefVdsCloud}
      --regions-exclude ${Input.Google.regionsExclude}
      --variant-qc-out ${paramsArr.harmRefFiltVariantQcCloud}
      --variants-prunedin-out ${paramsArr.harmRefFiltVariantsPrunedInCloud}
      --filt-vds-out ${paramsArr.harmRefFiltVdsCloud}
      --filt-plink-out ${paramsArr.harmRefFiltNameCloud}
      --filt-pruned-vds-out ${paramsArr.harmRefFiltPrunedVdsCloud}
      --filt-pruned-plink-out ${paramsArr.harmRefFiltPrunedNameCloud}"""
      .in(paramsArr.harmRefVdsCloud, Input.Google.regionsExclude)
      .out(((paramsArr.harmRefFiltCloud :+ paramsArr.harmRefFiltVdsCloud) ++ (paramsArr.harmRefFiltPrunedCloud :+ paramsArr.harmRefFiltPrunedVdsCloud)) :+ paramsArr.harmRefFiltVariantQcCloud :+ paramsArr.harmRefFiltVariantsPrunedInCloud)
  }
  
  local {
    googleCopy(paramsArr.harmRefFiltPrunedCloud, paramsArr.harmRefFiltPruned)
  }
  
  /**
   * Kinship Step
   *  Description: Calculate kinship to identify duplicates and any samples exhibiting abnormal (excessive) sharing
   *  Requires: King, R
   *  Notes:
   *     King is preferred to Plink or Hail based IBD calcs due to robust algorithm handling of population stratification. This step should be followed by a visual inspection for duplicates or excessive sharing
   * King only writes the '.kin0' file if families are found, so a bash script is used to write an empty file in that case
   */
  
  //uger {
  //  cmd"""$shKing $binKing ${paramsArr.harmRefFiltPrunedName}.bed ${paramsArr.kinName} ${paramsArr.kinLog} ${paramsArr.kinKin0} ${paramsArr.kinKin0Related}"""
  //  .in(paramsArr.harmRefFiltPruned)
  //  .out(paramsArr.kinLog, paramsArr.kinKin, paramsArr.kinTmpDat, paramsArr.kinTmpPed, paramsArr.kinKin0, paramsArr.kinKin0Related)
  //
  //  cmd"""$binR --vanilla --args ${paramsArr.kinKin0Related} ${paramsArr.kinFamsizes} < ${rCalcKinshipFamSizes}"""
  //  .in(paramsArr.kinKin0Related)
  //  .out(paramsArr.kinFamsizes)
  //}
  
  /**
    * Ancestry PCA Step
    *  Description: Calculate PCs combined with 1KG Phase 3 Purcell 5k data
    *  Requires: Hail, R, $rPlotAncestryPca
    *  Notes:
    *     To perform ancestry inference and clustering with 1KG data, we must combine on common variants with reference data (clustering does not work when only using PCA loadings and projecting)
    */
  
  google {
    hail"""$pyHailAncestryPcaMerge1kg
      --vds-in ${paramsArr.harmRefVdsCloud}
      --kg-vcf-in ${Input.Google.kgPurcellVcf}
      --kg-sample ${Input.Google.kgSample}
      --plink-out ${paramsArr.harmRef1kgNameCloud}"""
      .in(paramsArr.harmRefVdsCloud, Input.Google.kgPurcellVcf, Input.Google.kgSample)
      .out(paramsArr.harmRef1kgCloud)
  }
  
  local {
    googleCopy(paramsArr.harmRef1kgCloud, paramsArr.harmRef1kg)
  }
  
  uger {
    cmd"""$binRscript --vanilla --verbose
      $rPcair
      --plink-in ${paramsArr.harmRef1kgName}
      --gds-out ${paramsArr.harmRef1kgGds}
      --scores ${paramsArr.ancestryPcaScores}
      --id $projectId
      --force-unrel $kgSampleId ${Input.Local.kgSample}
      --update-pop $kgSampleId $kgSamplePop ${Input.Local.kgSample}
      --update-group $kgSampleId $kgSampleGroup ${Input.Local.kgSample}
      > ${paramsArr.ancestryPcaLog}"""
      .in(paramsArr.harmRef1kg :+ Input.Local.kgSample)
      .out(paramsArr.harmRef1kgGds, paramsArr.ancestryPcaLog, paramsArr.ancestryPcaScores)
      .using("R-3.4")
  
    cmd"""$binR --vanilla --args $projectId ${paramsArr.ancestryPcaScores} ${paramsArr.ancestryPcaScoresPlots} < $rPlotAncestryPca"""
    .in(paramsArr.ancestryPcaScores)
    .out(paramsArr.ancestryPcaScoresPlots)
  }
  
  /**
   * Ancestry Cluster Step
   *  Description: Cluster with 1KG samples using Gaussian Mixture Modeling and infer ancestry
   *  Requires: Hail, R
   *  Notes:
   *     *.ancestry.inferred.tsv contains the final inferred ancestry for each sample, including OUTLIERS
   *     This file is array specific
   */
  
  uger {
    cmd"""(echo 3; sed '1d' ${paramsArr.ancestryPcaScores} | cut -f4-6 | sed 's/\t/ /g') > ${paramsArr.ancestryClusterFet}"""
    .in(paramsArr.ancestryPcaScores)
    .out(paramsArr.ancestryClusterFet)
  
    cmd"""$binKlustakwik ${paramsArr.ancestryClusterName} 1 -UseFeatures 111 -UseDistributional 0 > ${paramsArr.ancestryClusterLog}"""
    .in(paramsArr.ancestryClusterFet)
    .out(paramsArr.ancestryClusterClu, paramsArr.ancestryClusterKlg, paramsArr.ancestryClusterLog)
  
    cmd"""$binR --vanilla --args ${paramsArr.ancestryPcaScores} ${paramsArr.ancestryClusterClu} ${Input.Local.pheno} $projectId $phenoId $phenoSrRace
      ${paramsArr.ancestryClusterPlots} ${paramsArr.ancestryClusterXtabs} ${paramsArr.ancestryClusterPlotsCenters}
      ${paramsArr.ancestryClusterGroups} ${paramsArr.ancestryInferred}
      ${paramsArr.ancestryClusterPlotsNo1kg} < $rPlotAncestryCluster"""
      .in(paramsArr.ancestryPcaScores, paramsArr.ancestryClusterClu, Input.Local.pheno)
      .out(paramsArr.ancestryClusterPlots, paramsArr.ancestryClusterXtabs, paramsArr.ancestryClusterPlotsCenters, paramsArr.ancestryClusterGroups, paramsArr.ancestryInferred, paramsArr.ancestryClusterPlotsNo1kg)
  }

  paramsArr.ancestryInferred

}

def qc2(i: Int): Unit = {

  val (_, paramsArr) = Params.paramsByArr(i)

  /**
   * PCA Step
   *  Description: Calculate PCs for all non-outlier samples combined (to be used for adjustment during sample outlier removal)
   *  Requires: R
   */
  
  uger {
  
    cmd"""$binRscript --vanilla --verbose
      $rPcair
      --plink-in ${paramsArr.harmRefFiltPrunedName}
      --gds-out ${paramsArr.harmRefFiltPrunedPcaGds}
      --exclude ${ancestryInferredMergedOutliers}
      --ancestry ${ancestryInferredMerged}
      --id $projectId
      --scores ${paramsArr.pcaScores}
      > ${paramsArr.pcaLog}"""
      .in(paramsArr.harmRefFiltPruned :+ ancestryInferredMerged :+ ancestryInferredMergedOutliers)
      .out(paramsArr.harmRefFiltPrunedPcaGds, paramsArr.pcaLog, paramsArr.pcaScores)
      .using("R-3.4")
  }
  
  /**
   * Sample QC Stats Calculation Step
   *  Description: Calculate sexcheck and sample by variant statistics for all samples
   *  Requires: Hail, R
   */
  
  local {
    googleCopy(ancestryInferredMerged, ancestryInferredMergedCloud)
    googleCopy(Input.Local.pheno, Input.Google.pheno)
  }
  
  google {
    hail"""$pyHailSexcheck
      --vds-in ${paramsArr.harmRefVdsCloud}
      --regions-exclude ${Input.Google.regionsExclude}
      --pheno-in ${Input.Google.pheno}
      --id-col $phenoId
      --sex-col $phenoSrSex
      --sexcheck-out ${paramsArr.sampleqcSexcheckCloud}
      --sexcheck-problems-out ${paramsArr.sampleqcSexcheckProblemsCloud}"""
      .in(Input.Google.pheno, paramsArr.harmRefVdsCloud, Input.Google.regionsExclude)
      .out(paramsArr.sampleqcSexcheckCloud, paramsArr.sampleqcSexcheckProblemsCloud)
  
    hail"""$pyHailSampleqc
      --vds-in ${paramsArr.harmRefFiltPrunedVdsCloud}
      --clusters-in ${ancestryInferredMergedCloud}
      --qc-out ${paramsArr.sampleqcStatsCloud}"""
      .in(paramsArr.harmRefFiltPrunedVdsCloud, ancestryInferredMergedCloud)
      .out(paramsArr.sampleqcStatsCloud)
  }
  
  local {
    googleCopy(paramsArr.sampleqcStatsCloud, paramsArr.sampleqcStats)
  }
  
  uger {
    cmd"""$binR --vanilla --args ${paramsArr.sampleqcStats} ${paramsArr.pcaScores} ${paramsArr.sampleqcStatsAdj} < $rCalcIstatsAdj"""
    .in(paramsArr.sampleqcStats, paramsArr.pcaScores)
    .out(paramsArr.sampleqcStatsAdj)
  
    cmd"""$binR --vanilla --args ${paramsArr.sampleqcStatsAdj} ${paramsArr.sampleqcStatsAdjCorrPlots} ${paramsArr.sampleqcStatsAdjPcaLoadings} ${paramsArr.sampleqcStatsAdjPcaScoresPlots} ${paramsArr.sampleqcStatsAdjPcaScores} < $rIstatsAdjPca"""
    .in(paramsArr.sampleqcStatsAdj)
    .out(paramsArr.sampleqcStatsAdjCorrPlots, paramsArr.sampleqcStatsAdjPcaLoadings, paramsArr.sampleqcStatsAdjPcaScoresPlots, paramsArr.sampleqcStatsAdjPcaScores)
  }
  
  local {
	googleCopy(paramsArr.sampleqcSexcheckCloud, paramsArr.sampleqcSexcheck)
    googleCopy(paramsArr.sampleqcSexcheckProblemsCloud, paramsArr.sampleqcSexcheckProblems)
  }
  
  /**
   * Sample QC PCA Clustering Step
   *  Description: Cluster PCs of adjusted sample QC metrics
   *  Requires: Klustakwik, R
   */
  
  uger {
    cmd"""N=$$(head -1 ${paramsArr.sampleqcStatsAdjPcaScores} | wc | awk '{print $$2-1}');
      echo $$N > ${paramsArr.sampleqcStatsAdjClusterFet};
      sed '1d' ${paramsArr.sampleqcStatsAdjPcaScores} | cut -f2- | sed 's/\t/ /g' >> ${paramsArr.sampleqcStatsAdjClusterFet};
      FEATURES=1; for i in $$(seq 2 $$N); do FEATURES=$${FEATURES}1; done;
      $binKlustakwik ${paramsArr.sampleqcStatsAdjClusterName} 1 -UseFeatures $$FEATURES -UseDistributional 0 >
      ${paramsArr.sampleqcStatsAdjClusterKlustakwikLog}"""
      .in(paramsArr.sampleqcStatsAdjClusterFet, paramsArr.sampleqcStatsAdjPcaScores)
      .out(paramsArr.sampleqcStatsAdjClusterClu, paramsArr.sampleqcStatsAdjClusterKlg, paramsArr.sampleqcStatsAdjClusterKlustakwikLog)
  
    cmd"""$binR --vanilla --args ${paramsArr.sampleqcStatsAdjPcaScores} ${paramsArr.sampleqcStatsAdjClusterClu}
      ${paramsArr.sampleqcStatsAdjClusterOutliers} ${paramsArr.sampleqcStatsAdjClusterPlots}
      ${paramsArr.sampleqcStatsAdjClusterXtabs} $projectId < $rIstatsPcsGmmClusterPlot"""
      .in(paramsArr.sampleqcStatsAdjPcaScores, paramsArr.sampleqcStatsAdjClusterClu)
      .out(paramsArr.sampleqcStatsAdjClusterOutliers, paramsArr.sampleqcStatsAdjClusterPlots, paramsArr.sampleqcStatsAdjClusterXtabs)
  
  }
  
  /**
   * Sample QC Individual Stats Clustering Step
   *  Description: Cluster PCs of adjusted sample QC metrics
   *  Requires: Klustakwik, R
   */
  
  for {
    (metric, paramsMetric) <- paramsArr.paramsByMetric
  } {
  
    uger {
      cmd"""echo 1 > ${paramsMetric.sampleqcStatsAdjIndClusterFet};
        metricIdx=`head -1 ${paramsArr.sampleqcStatsAdj} | tr '\t' '\n' | awk '{print NR" "$$0}' | grep -w ${paramsMetric.sampleqcStatsAdjIndMetric} | awk '{print $$1}'`;
        sed '1d' ${paramsArr.sampleqcStatsAdj} | awk -v col=$${metricIdx} '{print $$col}' >> ${paramsMetric.sampleqcStatsAdjIndClusterFet}"""
        .in(paramsArr.sampleqcStatsAdj)
        .out(paramsMetric.sampleqcStatsAdjIndClusterFet)
  
      cmd"""$binKlustakwik ${paramsMetric.sampleqcStatsAdjIndClusterName} 1 -UseFeatures 1 -UseDistributional 0 > ${paramsMetric.sampleqcStatsAdjIndClusterKlustakwikLog}"""
        .in(paramsMetric.sampleqcStatsAdjIndClusterFet)
        .out(paramsMetric.sampleqcStatsAdjIndClusterClu, paramsMetric.sampleqcStatsAdjIndClusterKlg, paramsMetric.sampleqcStatsAdjIndClusterKlustakwikLog)
    }
  
  }
  
  uger {
    cmd"""$binR --vanilla --args
      ${sampleQcMetrics.mkString(",")}
      ${paramsArr.sampleqcStats}
      ${paramsArr.sampleqcStatsAdj}
      ${paramsArr.sampleqcStatsAdjClusterOutliers}
      ${paramsArr.sampleqcStatsAdjIndBoxplots}
      ${paramsArr.sampleqcStatsAdjIndDiscreteness}
      ${paramsArr.sampleqcStatsAdjOutliersTable}
      ${paramsArr.sampleqcStatsAdjStripchart}
      ${ancestryInferredMerged}
      < $rIstatsAdjGmmPlotMetrics"""
      .in(paramsArr.paramsSortedByMetric.map(_.sampleqcStatsAdjIndClusterClu) :+ paramsArr.sampleqcStats :+ paramsArr.sampleqcStatsAdj :+ ancestryInferredMerged :+ paramsArr.sampleqcStatsAdjClusterOutliers)
      .out(paramsArr.sampleqcStatsAdjIndBoxplots, paramsArr.sampleqcStatsAdjIndDiscreteness, paramsArr.sampleqcStatsAdjOutliersTable, paramsArr.sampleqcStatsAdjStripchart)
  }
  
  /**
   * Compile Sample Exclusions Step
   * Requires: Python
   */
  
  uger {
  
    cmd"""python $pyCompileExclusions
      --ancestry-inferred ${ancestryInferredMerged}
      --kinship-related ${paramsArr.kinKin0Related}
      --kinship-famsizes ${paramsArr.kinFamsizes}
      --sampleqc-outliers ${paramsArr.sampleqcStatsAdjOutliersTable}
      --sexcheck-problems ${paramsArr.sampleqcSexcheckProblems}
      --ancestry-keep ${ancestryKeep.mkString(",")}
      --duplicates-keep ${duplicatesKeep.mkString(",")}
      --famsize-keep ${famsizeKeep.mkString(",")}
      --sampleqc-keep ${sampleqcKeep.mkString(",")}
      --sexcheck-keep ${sexcheckKeep.mkString(",")}
      --out ${paramsArr.finalSampleExclusions}"""
      .in(ancestryInferredMerged, paramsArr.kinKin0Related, paramsArr.kinFamsizes, paramsArr.sampleqcStatsAdjOutliersTable, paramsArr.sampleqcSexcheckProblems)
      .out(paramsArr.finalSampleExclusions)
  }

  /**
  * Filter Clean Step
  * filter variants and generate final clean dataset
  */

  local {
    googleCopy(paramsArr.finalSampleExclusions, paramsArr.finalSampleExclusionsCloud)
  }
  
  google {
    hail"""$pyHailFilterFinal
      --vds-in ${paramsArr.harmRefVdsCloud}
      --ancestry-in ${ancestryInferredMergedCloud}
      --sexcheck-in ${paramsArr.sampleqcSexcheckCloud}
      --pheno-in ${Input.Google.pheno}
      --iid-col $phenoId
      --case-ctrl-col $phenoStatus
      --samples-remove ${paramsArr.finalSampleExclusionsCloud}
      --variantqc-out ${paramsArr.variantqcStatsCloud}
      --variants-exclude-out ${paramsArr.finalVariantExclusionsCloud}
      --plink-out ${paramsArr.cleanNameCloud}
      --vcf-out ${paramsArr.cleanVcfCloud}
      --vds-out ${paramsArr.cleanVdsCloud}"""
      .in(paramsArr.harmRefVdsCloud, ancestryInferredMergedCloud, paramsArr.sampleqcSexcheckCloud, Input.Google.pheno, paramsArr.finalSampleExclusionsCloud)
      .out(paramsArr.cleanCloud :+ paramsArr.cleanVcfCloud :+ paramsArr.variantqcStatsCloud :+ paramsArr.finalVariantExclusionsCloud)
  }

  local {
    googleCopy(paramsArr.cleanCloud, paramsArr.clean)
    googleCopy(paramsArr.cleanVcfCloud, paramsArr.cleanVcf)
    googleCopy(paramsArr.cleanVdsCloud, paramsArr.cleanVds)
    googleCopy(paramsArr.variantqcStatsCloud, paramsArr.variantqcStats)
	googleCopy(paramsArr.finalVariantExclusionsCloud, paramsArr.finalVariantExclusions)
  }

  local {
    cmd"""$binTabix -f -p vcf ${paramsArr.cleanVcf}"""
      .in(paramsArr.cleanVcf)
      .out(paramsArr.cleanVcfTbi)
  }

}

