//
// The dumbest thing that could work: 
//   1) Copy a.txt to b.txt locally
//   2) Copy b.txt to c.txt on Uger 
//   3) Run Hail on the google cloud

val a = store.at("a.txt").asInput
val b = store.at("b.txt")
val c = store.at("c.txt")
val d = store

//TODO: Get this from loamstream.conf somehow
val gcloud = path("/home/unix/cgilbert/humgen/google-cloud-sdk/bin/gcloud")

uger {
  cmd"cp $a $b".in(a).out(b)
}

local {
  cmd"cp $b $c".in(b).out(c)
}

//Basically run 'hail --help'; stresses cluster creation and teardown, plus hail invocation
//TODO: Get cluster name and jar path from config file
google {
  cmd"$gcloud dataproc jobs submit spark --cluster cg-test --jar gs://loamstream/hail/hail-all-spark.jar_cloud --class org.broadinstitute.hail.driver.Main --help".in(c).out(d)
}
